{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "X = train_data.drop(columns=['id', 'yield'])\n",
    "y = train_data['yield']\n",
    "X_test = test_data.drop(columns=['id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "12 fits failed out of a total of 60.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "12 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of DecisionTreeRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [-461.42250033           nan -369.19664457           nan           nan\n",
      " -322.47610452 -443.16016051 -289.17739388           nan -361.00319628\n",
      " -357.56411172 -335.89938423 -287.69394809 -337.39590664 -300.12567921\n",
      " -322.47610452 -298.01490957 -335.90753918 -346.92019494 -330.59283504]\n",
      "  warnings.warn(\n",
      "INFO:__main__:Decision Tree MAE on validation set: 277.50\n",
      "INFO:__main__:Switching to Linear Regression as MAE target was not met.\n",
      "INFO:__main__:Linear Regression MAE on validation set: 7211486644.18\n",
      "INFO:__main__:Final model retrained on the entire training dataset.\n",
      "INFO:__main__:Predictions made on the test set.\n",
      "INFO:__main__:Submission file saved as 'submission_simplified_regression.csv'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "tree_model = DecisionTreeRegressor(random_state=42)\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('poly_features', PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', tree_model)\n",
    "])\n",
    "\n",
    "tree_params = {\n",
    "    'model__max_depth': [5, 10, 15, 20, None],\n",
    "    'model__min_samples_split': [2, 5, 10],\n",
    "    'model__min_samples_leaf': [1, 2, 4],\n",
    "    'model__max_features': ['auto', 'sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_distributions=tree_params,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    n_iter=20, \n",
    "    cv=3,\n",
    "    random_state=42\n",
    ")\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "best_pipeline = search.best_estimator_\n",
    "y_val_pred = best_pipeline.predict(X_val)\n",
    "mae = mean_absolute_error(y_val, y_val_pred)\n",
    "logger.info(f\"Decision Tree MAE on validation set: {mae:.2f}\")\n",
    "\n",
    "if mae > 220:\n",
    "    logger.info(\"Switching to Linear Regression as MAE target was not met.\")\n",
    "    linear_model_pipeline = Pipeline([\n",
    "        ('poly_features', PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', linear_model)\n",
    "    ])\n",
    "    linear_model_pipeline.fit(X_train, y_train)\n",
    "    y_val_pred = linear_model_pipeline.predict(X_val)\n",
    "    mae = mean_absolute_error(y_val, y_val_pred)\n",
    "    logger.info(f\"Linear Regression MAE on validation set: {mae:.2f}\")\n",
    "    best_pipeline = linear_model_pipeline if mae <= 220 else search.best_estimator_\n",
    "\n",
    "# Retrain best model on full training data\n",
    "best_pipeline.fit(X, y)\n",
    "logger.info(\"Final model retrained on the entire training dataset.\")\n",
    "\n",
    "# Predictions on the test set\n",
    "y_test_pred = best_pipeline.predict(X_test)\n",
    "logger.info(\"Predictions made on the test set.\")\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_data['id'],\n",
    "    'yield': y_test_pred\n",
    "})\n",
    "submission.to_csv('submission_simplified_regression.csv', index=False)\n",
    "logger.info(\"Submission file saved as 'submission_simplified_regression.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-10 07:21:10,783] A new study created in memory with name: no-name-515f365d-3ba3-42a4-9394-4c3eace4be34\n",
      "[I 2024-11-10 07:22:45,665] Trial 0 finished with value: 272.12263076657393 and parameters: {'enet_alpha': 0.0005734309182864712, 'enet_l1_ratio': 0.6963348233789081, 'svr_C': 5.592970312792988, 'svr_epsilon': 0.8503780591251312, 'knn_neighbors': 12, 'dt_max_depth': 20, 'dt_min_samples_split': 5, 'rf_n_estimators': 114, 'rf_max_depth': 22}. Best is trial 0 with value: 272.12263076657393.\n",
      "[I 2024-11-10 07:24:35,281] Trial 1 finished with value: 273.50315481420085 and parameters: {'enet_alpha': 0.011275410001108778, 'enet_l1_ratio': 0.12667316129444242, 'svr_C': 4.165323933152529, 'svr_epsilon': 0.8502305837408295, 'knn_neighbors': 6, 'dt_max_depth': 24, 'dt_min_samples_split': 10, 'rf_n_estimators': 196, 'rf_max_depth': 45}. Best is trial 0 with value: 272.12263076657393.\n",
      "[I 2024-11-10 07:25:36,524] Trial 2 finished with value: 298.9129045736579 and parameters: {'enet_alpha': 0.18351345019500576, 'enet_l1_ratio': 0.30347129818398944, 'svr_C': 0.9523088574613991, 'svr_epsilon': 0.04330707254073715, 'knn_neighbors': 18, 'dt_max_depth': 24, 'dt_min_samples_split': 9, 'rf_n_estimators': 83, 'rf_max_depth': 38}. Best is trial 0 with value: 272.12263076657393.\n",
      "[I 2024-11-10 07:26:22,573] Trial 3 finished with value: 301.7310333263301 and parameters: {'enet_alpha': 0.36900677389393743, 'enet_l1_ratio': 0.5165052001519616, 'svr_C': 0.743961083562482, 'svr_epsilon': 0.017903591449340544, 'knn_neighbors': 11, 'dt_max_depth': 14, 'dt_min_samples_split': 8, 'rf_n_estimators': 57, 'rf_max_depth': 17}. Best is trial 0 with value: 272.12263076657393.\n",
      "[I 2024-11-10 07:27:38,541] Trial 4 finished with value: 274.57142568190955 and parameters: {'enet_alpha': 0.11496139732377417, 'enet_l1_ratio': 0.7212967188327328, 'svr_C': 5.878992000901455, 'svr_epsilon': 0.03285824073939167, 'knn_neighbors': 11, 'dt_max_depth': 27, 'dt_min_samples_split': 3, 'rf_n_estimators': 121, 'rf_max_depth': 29}. Best is trial 0 with value: 272.12263076657393.\n",
      "[I 2024-11-10 07:28:58,741] Trial 5 finished with value: 308.9552244098977 and parameters: {'enet_alpha': 0.00038324431883288027, 'enet_l1_ratio': 0.6750417459588031, 'svr_C': 0.177570249150056, 'svr_epsilon': 0.2833479035360042, 'knn_neighbors': 5, 'dt_max_depth': 9, 'dt_min_samples_split': 4, 'rf_n_estimators': 109, 'rf_max_depth': 16}. Best is trial 0 with value: 272.12263076657393.\n",
      "[I 2024-11-10 07:30:09,061] Trial 6 finished with value: 292.29705881903726 and parameters: {'enet_alpha': 0.4440879432672012, 'enet_l1_ratio': 0.49234537826455904, 'svr_C': 1.4652992509087746, 'svr_epsilon': 0.056717908225674164, 'knn_neighbors': 6, 'dt_max_depth': 28, 'dt_min_samples_split': 5, 'rf_n_estimators': 95, 'rf_max_depth': 29}. Best is trial 0 with value: 272.12263076657393.\n",
      "[I 2024-11-10 07:30:59,031] Trial 7 finished with value: 276.6310366450319 and parameters: {'enet_alpha': 0.02708994586735259, 'enet_l1_ratio': 0.39051591384802364, 'svr_C': 3.384077897432498, 'svr_epsilon': 0.017257166423796017, 'knn_neighbors': 9, 'dt_max_depth': 23, 'dt_min_samples_split': 7, 'rf_n_estimators': 54, 'rf_max_depth': 48}. Best is trial 0 with value: 272.12263076657393.\n",
      "[I 2024-11-10 07:32:01,161] Trial 8 finished with value: 318.9230504034367 and parameters: {'enet_alpha': 0.4393013532125032, 'enet_l1_ratio': 0.8563684035441893, 'svr_C': 0.110362917208777, 'svr_epsilon': 0.24837685477509372, 'knn_neighbors': 7, 'dt_max_depth': 27, 'dt_min_samples_split': 4, 'rf_n_estimators': 110, 'rf_max_depth': 15}. Best is trial 0 with value: 272.12263076657393.\n",
      "[I 2024-11-10 07:33:45,331] Trial 9 finished with value: 311.8187845843696 and parameters: {'enet_alpha': 0.12841071715782534, 'enet_l1_ratio': 0.4318721166956917, 'svr_C': 0.2925200752713121, 'svr_epsilon': 0.0509450862739291, 'knn_neighbors': 15, 'dt_max_depth': 10, 'dt_min_samples_split': 9, 'rf_n_estimators': 196, 'rf_max_depth': 24}. Best is trial 0 with value: 272.12263076657393.\n",
      "[I 2024-11-10 07:35:34,718] Trial 10 finished with value: 265.44191848383537 and parameters: {'enet_alpha': 0.0004164027547631361, 'enet_l1_ratio': 0.8940140679512614, 'svr_C': 42.63281831637687, 'svr_epsilon': 0.8606353237278725, 'knn_neighbors': 15, 'dt_max_depth': 18, 'dt_min_samples_split': 2, 'rf_n_estimators': 155, 'rf_max_depth': 36}. Best is trial 10 with value: 265.44191848383537.\n",
      "[I 2024-11-10 07:37:13,579] Trial 11 finished with value: 264.87636700776875 and parameters: {'enet_alpha': 0.0002990589897924834, 'enet_l1_ratio': 0.895700195387007, 'svr_C': 52.92121387847585, 'svr_epsilon': 0.5761059664960679, 'knn_neighbors': 15, 'dt_max_depth': 18, 'dt_min_samples_split': 2, 'rf_n_estimators': 155, 'rf_max_depth': 39}. Best is trial 11 with value: 264.87636700776875.\n",
      "[I 2024-11-10 07:38:58,460] Trial 12 finished with value: 263.83443896899917 and parameters: {'enet_alpha': 0.00139781922241276, 'enet_l1_ratio': 0.8757834343597579, 'svr_C': 85.8118650921114, 'svr_epsilon': 0.29899679176957855, 'knn_neighbors': 16, 'dt_max_depth': 16, 'dt_min_samples_split': 2, 'rf_n_estimators': 161, 'rf_max_depth': 39}. Best is trial 12 with value: 263.83443896899917.\n",
      "[I 2024-11-10 07:40:41,075] Trial 13 finished with value: 262.8735363261115 and parameters: {'enet_alpha': 0.00010396588403656479, 'enet_l1_ratio': 0.797980579418127, 'svr_C': 96.94775091804891, 'svr_epsilon': 0.26403132698602283, 'knn_neighbors': 18, 'dt_max_depth': 14, 'dt_min_samples_split': 2, 'rf_n_estimators': 158, 'rf_max_depth': 39}. Best is trial 13 with value: 262.8735363261115.\n",
      "[I 2024-11-10 07:42:21,766] Trial 14 finished with value: 264.4519514308853 and parameters: {'enet_alpha': 0.001744102709728124, 'enet_l1_ratio': 0.7728406146005695, 'svr_C': 24.694858940899685, 'svr_epsilon': 0.1586513466586349, 'knn_neighbors': 20, 'dt_max_depth': 5, 'dt_min_samples_split': 2, 'rf_n_estimators': 155, 'rf_max_depth': 44}. Best is trial 13 with value: 262.8735363261115.\n",
      "[I 2024-11-10 07:44:07,838] Trial 15 finished with value: 263.0268395220767 and parameters: {'enet_alpha': 0.00011840042614997743, 'enet_l1_ratio': 0.6149569332098677, 'svr_C': 91.23684684194187, 'svr_epsilon': 0.10894924032354521, 'knn_neighbors': 18, 'dt_max_depth': 14, 'dt_min_samples_split': 3, 'rf_n_estimators': 170, 'rf_max_depth': 34}. Best is trial 13 with value: 262.8735363261115.\n",
      "[I 2024-11-10 07:45:56,638] Trial 16 finished with value: 267.23404213753565 and parameters: {'enet_alpha': 0.00012686121214117174, 'enet_l1_ratio': 0.611457977685248, 'svr_C': 14.595505636584893, 'svr_epsilon': 0.09291647699679695, 'knn_neighbors': 20, 'dt_max_depth': 12, 'dt_min_samples_split': 4, 'rf_n_estimators': 178, 'rf_max_depth': 34}. Best is trial 13 with value: 262.8735363261115.\n",
      "[I 2024-11-10 07:47:26,045] Trial 17 finished with value: 265.4688666712303 and parameters: {'enet_alpha': 0.0032773144773619828, 'enet_l1_ratio': 0.5880080863725143, 'svr_C': 20.22361094435017, 'svr_epsilon': 0.12742193248856346, 'knn_neighbors': 18, 'dt_max_depth': 8, 'dt_min_samples_split': 6, 'rf_n_estimators': 135, 'rf_max_depth': 33}. Best is trial 13 with value: 262.8735363261115.\n",
      "[I 2024-11-10 07:48:31,146] Trial 18 finished with value: 262.99276079390955 and parameters: {'enet_alpha': 0.00010344225441240686, 'enet_l1_ratio': 0.5877706291719724, 'svr_C': 98.09407099776368, 'svr_epsilon': 0.09034013865805307, 'knn_neighbors': 18, 'dt_max_depth': 14, 'dt_min_samples_split': 3, 'rf_n_estimators': 137, 'rf_max_depth': 10}. Best is trial 13 with value: 262.8735363261115.\n",
      "[I 2024-11-10 07:49:36,518] Trial 19 finished with value: 266.1215693807766 and parameters: {'enet_alpha': 0.00010166714322408191, 'enet_l1_ratio': 0.7892143220711648, 'svr_C': 13.298970377767473, 'svr_epsilon': 0.3940868070038424, 'knn_neighbors': 17, 'dt_max_depth': 6, 'dt_min_samples_split': 3, 'rf_n_estimators': 138, 'rf_max_depth': 10}. Best is trial 13 with value: 262.8735363261115.\n",
      "[I 2024-11-10 07:51:00,697] Trial 20 finished with value: 264.3494682648338 and parameters: {'enet_alpha': 0.011334509569831197, 'enet_l1_ratio': 0.2736334645239789, 'svr_C': 38.54486181800558, 'svr_epsilon': 0.185296081843691, 'knn_neighbors': 13, 'dt_max_depth': 12, 'dt_min_samples_split': 6, 'rf_n_estimators': 142, 'rf_max_depth': 24}. Best is trial 13 with value: 262.8735363261115.\n",
      "[I 2024-11-10 07:52:49,773] Trial 21 finished with value: 263.46660197609526 and parameters: {'enet_alpha': 0.00017300721066351368, 'enet_l1_ratio': 0.601632559551621, 'svr_C': 87.33353249233915, 'svr_epsilon': 0.08484153050384767, 'knn_neighbors': 19, 'dt_max_depth': 15, 'dt_min_samples_split': 3, 'rf_n_estimators': 175, 'rf_max_depth': 42}. Best is trial 13 with value: 262.8735363261115.\n",
      "[I 2024-11-10 07:54:38,716] Trial 22 finished with value: 262.4702411508922 and parameters: {'enet_alpha': 0.000988322895840345, 'enet_l1_ratio': 0.5283965293046562, 'svr_C': 88.24691835860857, 'svr_epsilon': 0.07194149204761849, 'knn_neighbors': 17, 'dt_max_depth': 12, 'dt_min_samples_split': 3, 'rf_n_estimators': 176, 'rf_max_depth': 31}. Best is trial 22 with value: 262.4702411508922.\n",
      "[I 2024-11-10 07:56:31,795] Trial 23 finished with value: 263.72161235841764 and parameters: {'enet_alpha': 0.0010471355701888053, 'enet_l1_ratio': 0.5356249850811879, 'svr_C': 43.85400850081967, 'svr_epsilon': 0.03087341733943652, 'knn_neighbors': 17, 'dt_max_depth': 12, 'dt_min_samples_split': 5, 'rf_n_estimators': 186, 'rf_max_depth': 27}. Best is trial 22 with value: 262.4702411508922.\n",
      "[I 2024-11-10 07:57:35,341] Trial 24 finished with value: 263.64556127984685 and parameters: {'enet_alpha': 0.004155268146491774, 'enet_l1_ratio': 0.35037818662194353, 'svr_C': 98.98963195251811, 'svr_epsilon': 0.07192793385977177, 'knn_neighbors': 14, 'dt_max_depth': 20, 'dt_min_samples_split': 4, 'rf_n_estimators': 142, 'rf_max_depth': 11}. Best is trial 22 with value: 262.4702411508922.\n",
      "[I 2024-11-10 07:58:58,523] Trial 25 finished with value: 267.7858195587588 and parameters: {'enet_alpha': 0.0007358011082034092, 'enet_l1_ratio': 0.4587921190255096, 'svr_C': 12.330240473818257, 'svr_epsilon': 0.010749975717323444, 'knn_neighbors': 19, 'dt_max_depth': 11, 'dt_min_samples_split': 3, 'rf_n_estimators': 126, 'rf_max_depth': 19}. Best is trial 22 with value: 262.4702411508922.\n",
      "[I 2024-11-10 08:00:45,310] Trial 26 finished with value: 263.97367927365747 and parameters: {'enet_alpha': 0.0002776099718251805, 'enet_l1_ratio': 0.7854566350187591, 'svr_C': 26.437168466330977, 'svr_epsilon': 0.17726107489840404, 'knn_neighbors': 16, 'dt_max_depth': 7, 'dt_min_samples_split': 2, 'rf_n_estimators': 167, 'rf_max_depth': 31}. Best is trial 22 with value: 262.4702411508922.\n",
      "[I 2024-11-10 08:02:54,432] Trial 27 finished with value: 264.8385937720791 and parameters: {'enet_alpha': 0.003253207350774477, 'enet_l1_ratio': 0.6504606349224521, 'svr_C': 53.93024266586915, 'svr_epsilon': 0.47112513132468387, 'knn_neighbors': 17, 'dt_max_depth': 16, 'dt_min_samples_split': 4, 'rf_n_estimators': 182, 'rf_max_depth': 26}. Best is trial 22 with value: 262.4702411508922.\n",
      "[I 2024-11-10 08:04:44,977] Trial 28 finished with value: 270.54005630824753 and parameters: {'enet_alpha': 0.00019960881501367684, 'enet_l1_ratio': 0.5544520617966554, 'svr_C': 7.645588849362037, 'svr_epsilon': 0.13455901248259586, 'knn_neighbors': 20, 'dt_max_depth': 13, 'dt_min_samples_split': 3, 'rf_n_estimators': 151, 'rf_max_depth': 41}. Best is trial 22 with value: 262.4702411508922.\n",
      "[I 2024-11-10 08:06:58,842] Trial 29 finished with value: 281.20799189783895 and parameters: {'enet_alpha': 0.00047585450777391986, 'enet_l1_ratio': 0.727584901138823, 'svr_C': 2.0157969096542705, 'svr_epsilon': 0.06397521995746341, 'knn_neighbors': 13, 'dt_max_depth': 21, 'dt_min_samples_split': 6, 'rf_n_estimators': 129, 'rf_max_depth': 50}. Best is trial 22 with value: 262.4702411508922.\n",
      "[I 2024-11-10 08:09:40,729] Trial 30 finished with value: 264.3803827135033 and parameters: {'enet_alpha': 0.0007340160920934664, 'enet_l1_ratio': 0.8230321599029563, 'svr_C': 64.03303227206189, 'svr_epsilon': 0.20764524158536446, 'knn_neighbors': 19, 'dt_max_depth': 17, 'dt_min_samples_split': 5, 'rf_n_estimators': 185, 'rf_max_depth': 21}. Best is trial 22 with value: 262.4702411508922.\n",
      "[I 2024-11-10 08:12:17,711] Trial 31 finished with value: 265.8283048547765 and parameters: {'enet_alpha': 0.00010805358487105419, 'enet_l1_ratio': 0.639134038094592, 'svr_C': 28.877935350607927, 'svr_epsilon': 0.11059854208923561, 'knn_neighbors': 18, 'dt_max_depth': 14, 'dt_min_samples_split': 3, 'rf_n_estimators': 166, 'rf_max_depth': 35}. Best is trial 22 with value: 262.4702411508922.\n",
      "[I 2024-11-10 08:14:38,745] Trial 32 finished with value: 261.81798637882497 and parameters: {'enet_alpha': 0.0002356458967617026, 'enet_l1_ratio': 0.4795313002068968, 'svr_C': 97.99626306923692, 'svr_epsilon': 0.03875643065382713, 'knn_neighbors': 16, 'dt_max_depth': 10, 'dt_min_samples_split': 2, 'rf_n_estimators': 169, 'rf_max_depth': 32}. Best is trial 32 with value: 261.81798637882497.\n",
      "[I 2024-11-10 08:17:45,510] Trial 33 finished with value: 262.71369860217885 and parameters: {'enet_alpha': 0.0002401505741539444, 'enet_l1_ratio': 0.20431985232710265, 'svr_C': 64.52843582163852, 'svr_epsilon': 0.039088545992425956, 'knn_neighbors': 16, 'dt_max_depth': 10, 'dt_min_samples_split': 2, 'rf_n_estimators': 199, 'rf_max_depth': 31}. Best is trial 32 with value: 261.81798637882497.\n",
      "[I 2024-11-10 08:21:09,964] Trial 34 finished with value: 262.8223685629813 and parameters: {'enet_alpha': 0.00022394436337766903, 'enet_l1_ratio': 0.10136405648815197, 'svr_C': 58.361950820679475, 'svr_epsilon': 0.030973310031515718, 'knn_neighbors': 16, 'dt_max_depth': 10, 'dt_min_samples_split': 2, 'rf_n_estimators': 200, 'rf_max_depth': 32}. Best is trial 32 with value: 261.81798637882497.\n",
      "[I 2024-11-10 08:24:26,687] Trial 35 finished with value: 263.82627659948383 and parameters: {'enet_alpha': 0.00023324552713286487, 'enet_l1_ratio': 0.12191735591266456, 'svr_C': 32.42033591263355, 'svr_epsilon': 0.03431322041912618, 'knn_neighbors': 14, 'dt_max_depth': 9, 'dt_min_samples_split': 2, 'rf_n_estimators': 200, 'rf_max_depth': 31}. Best is trial 32 with value: 261.81798637882497.\n",
      "[I 2024-11-10 08:27:40,100] Trial 36 finished with value: 262.95357766904027 and parameters: {'enet_alpha': 0.0006368108096961968, 'enet_l1_ratio': 0.19714737452097847, 'svr_C': 59.450598817345806, 'svr_epsilon': 0.023275742150634184, 'knn_neighbors': 16, 'dt_max_depth': 10, 'dt_min_samples_split': 2, 'rf_n_estimators': 192, 'rf_max_depth': 32}. Best is trial 32 with value: 261.81798637882497.\n",
      "[I 2024-11-10 08:30:41,794] Trial 37 finished with value: 280.87698172765386 and parameters: {'enet_alpha': 0.9362376791398924, 'enet_l1_ratio': 0.19012921038283198, 'svr_C': 7.853182859751097, 'svr_epsilon': 0.04075295867955317, 'knn_neighbors': 14, 'dt_max_depth': 8, 'dt_min_samples_split': 10, 'rf_n_estimators': 188, 'rf_max_depth': 28}. Best is trial 32 with value: 261.81798637882497.\n",
      "[I 2024-11-10 08:34:09,998] Trial 38 finished with value: 261.31511085150106 and parameters: {'enet_alpha': 0.002063866342526617, 'enet_l1_ratio': 0.18788533283846015, 'svr_C': 67.37979698602409, 'svr_epsilon': 0.023562797607591564, 'knn_neighbors': 11, 'dt_max_depth': 5, 'dt_min_samples_split': 2, 'rf_n_estimators': 200, 'rf_max_depth': 37}. Best is trial 38 with value: 261.31511085150106.\n",
      "[I 2024-11-10 08:35:49,953] Trial 39 finished with value: 265.04136807582853 and parameters: {'enet_alpha': 0.007600424592639547, 'enet_l1_ratio': 0.2554689107720721, 'svr_C': 17.040050145927214, 'svr_epsilon': 0.021295378217058875, 'knn_neighbors': 10, 'dt_max_depth': 5, 'dt_min_samples_split': 7, 'rf_n_estimators': 75, 'rf_max_depth': 37}. Best is trial 38 with value: 261.31511085150106.\n",
      "[I 2024-11-10 08:38:37,731] Trial 40 finished with value: 302.5155554717101 and parameters: {'enet_alpha': 0.03311985643044303, 'enet_l1_ratio': 0.1728878733051423, 'svr_C': 0.4788534611688005, 'svr_epsilon': 0.011172317730255926, 'knn_neighbors': 12, 'dt_max_depth': 7, 'dt_min_samples_split': 5, 'rf_n_estimators': 173, 'rf_max_depth': 25}. Best is trial 38 with value: 261.31511085150106.\n",
      "[I 2024-11-10 08:42:00,155] Trial 41 finished with value: 262.2378935953524 and parameters: {'enet_alpha': 0.001541966678893901, 'enet_l1_ratio': 0.10087592218181662, 'svr_C': 64.6711106356435, 'svr_epsilon': 0.026392294444945636, 'knn_neighbors': 9, 'dt_max_depth': 10, 'dt_min_samples_split': 2, 'rf_n_estimators': 199, 'rf_max_depth': 30}. Best is trial 38 with value: 261.31511085150106.\n",
      "[I 2024-11-10 08:45:11,489] Trial 42 finished with value: 261.5693866400554 and parameters: {'enet_alpha': 0.0020702239327698786, 'enet_l1_ratio': 0.15667588330333376, 'svr_C': 67.71378980851415, 'svr_epsilon': 0.014199801223219661, 'knn_neighbors': 8, 'dt_max_depth': 8, 'dt_min_samples_split': 3, 'rf_n_estimators': 192, 'rf_max_depth': 29}. Best is trial 38 with value: 261.31511085150106.\n",
      "[I 2024-11-10 08:48:22,677] Trial 43 finished with value: 262.82252767689124 and parameters: {'enet_alpha': 0.002333232848740788, 'enet_l1_ratio': 0.15518573333022032, 'svr_C': 31.33027119427236, 'svr_epsilon': 0.014232987310435855, 'knn_neighbors': 8, 'dt_max_depth': 7, 'dt_min_samples_split': 3, 'rf_n_estimators': 192, 'rf_max_depth': 29}. Best is trial 38 with value: 261.31511085150106.\n",
      "[I 2024-11-10 08:51:11,104] Trial 44 finished with value: 262.8576319876996 and parameters: {'enet_alpha': 0.0072388858051357675, 'enet_l1_ratio': 0.2550005982287089, 'svr_C': 43.36933929922401, 'svr_epsilon': 0.023510673748134703, 'knn_neighbors': 9, 'dt_max_depth': 8, 'dt_min_samples_split': 3, 'rf_n_estimators': 180, 'rf_max_depth': 22}. Best is trial 38 with value: 261.31511085150106.\n",
      "[W 2024-11-10 08:52:46,071] Trial 45 failed with parameters: {'enet_alpha': 0.0011463992050321507, 'enet_l1_ratio': 0.31222547397538586, 'svr_C': 60.90566898116349, 'svr_epsilon': 0.01660360965224642, 'knn_neighbors': 11, 'dt_max_depth': 30, 'dt_min_samples_split': 4, 'rf_n_estimators': 191, 'rf_max_depth': 29} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_22368\\2565442825.py\", line 97, in objective\n",
      "    mae_scores = cross_val_score(pipeline, X, y, cv=cv, scoring=make_scorer(mean_absolute_error), n_jobs=-1)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 213, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 712, in cross_val_score\n",
      "    cv_results = cross_validate(\n",
      "                 ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 213, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 423, in cross_validate\n",
      "    results = parallel(\n",
      "              ^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 74, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 2007, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 1650, in _get_outputs\n",
      "    yield from self._retrieve()\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 1762, in _retrieve\n",
      "    time.sleep(0.01)\n",
      "KeyboardInterrupt\n",
      "[W 2024-11-10 08:52:46,074] Trial 45 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 102\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Optimize model using Optuna\u001b[39;00m\n\u001b[0;32m    101\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 102\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# Retrieve the best parameters and model\u001b[39;00m\n\u001b[0;32m    105\u001b[0m best_trial \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_trial\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    247\u001b[0m ):\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[8], line 97\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# Cross-validation for MAE scoring\u001b[39;00m\n\u001b[0;32m     96\u001b[0m cv \u001b[38;5;241m=\u001b[39m KFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 97\u001b[0m mae_scores \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_scorer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean_absolute_error\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(mae_scores)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:712\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    710\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[1;32m--> 712\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    725\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:423\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    422\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 423\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    443\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, RobustScaler\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "from sklearn.linear_model import ElasticNet, Ridge, Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, VotingRegressor, StackingRegressor\n",
    "import optuna\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "target_column = 'yield'\n",
    "X = train_data.drop(columns=['id', target_column])\n",
    "y = train_data[target_column]\n",
    "X_test = test_data.drop(columns=['id'])\n",
    "\n",
    "if 'seeds' in X.columns and 'fruitmass' in X.columns:\n",
    "    X['fruit_density'] = X['seeds'] / (X['fruitmass'] + 1e-3)\n",
    "    X_test['fruit_density'] = X_test['seeds'] / (X_test['fruitmass'] + 1e-3)\n",
    "\n",
    "if 'clonesize' in X.columns and 'seeds' in X.columns:\n",
    "    X['seeds_clonesize_interaction'] = X['seeds'] * X['clonesize']\n",
    "    X_test['seeds_clonesize_interaction'] = X_test['seeds'] * X_test['clonesize']\n",
    "\n",
    "for col in ['fruitmass', 'seeds', 'RainingDays']:\n",
    "    if col in X.columns:\n",
    "        X[f'log_{col}'] = np.log1p(X[col])\n",
    "        X_test[f'log_{col}'] = np.log1p(X_test[col])\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "important_features = ['fruit_density', 'seeds_clonesize_interaction']\n",
    "X_poly = poly_features.fit_transform(X[important_features])\n",
    "X_poly_df = pd.DataFrame(X_poly, columns=poly_features.get_feature_names_out(important_features))\n",
    "X = pd.concat([X.reset_index(drop=True), X_poly_df], axis=1)\n",
    "\n",
    "X_test_poly = poly_features.transform(X_test[important_features])\n",
    "X_test_poly_df = pd.DataFrame(X_test_poly, columns=poly_features.get_feature_names_out(important_features))\n",
    "X_test = pd.concat([X_test.reset_index(drop=True), X_test_poly_df], axis=1)\n",
    "\n",
    "def objective(trial):\n",
    "    enet_alpha = trial.suggest_float('enet_alpha', 1e-4, 1.0, log=True)\n",
    "    enet_l1_ratio = trial.suggest_float('enet_l1_ratio', 0.1, 0.9)\n",
    "    \n",
    "    svr_C = trial.suggest_float('svr_C', 0.1, 100.0, log=True)\n",
    "    svr_epsilon = trial.suggest_float('svr_epsilon', 0.01, 1.0, log=True)\n",
    "    \n",
    "    knn_neighbors = trial.suggest_int('knn_neighbors', 5, 20)\n",
    "\n",
    "    dt_max_depth = trial.suggest_int('dt_max_depth', 5, 30)\n",
    "    dt_min_samples_split = trial.suggest_int('dt_min_samples_split', 2, 10)\n",
    "    \n",
    "    rf_n_estimators = trial.suggest_int('rf_n_estimators', 50, 200)\n",
    "    rf_max_depth = trial.suggest_int('rf_max_depth', 10, 50)\n",
    "\n",
    "    estimators = [\n",
    "        ('ridge', Ridge(alpha=1.0)),\n",
    "        ('lasso', Lasso(alpha=0.1)),\n",
    "        ('enet', ElasticNet(alpha=enet_alpha, l1_ratio=enet_l1_ratio, max_iter=10000)),\n",
    "        ('svr', SVR(C=svr_C, epsilon=svr_epsilon, kernel='rbf')),\n",
    "        ('knn', KNeighborsRegressor(n_neighbors=knn_neighbors)),\n",
    "        ('dt', DecisionTreeRegressor(max_depth=dt_max_depth, min_samples_split=dt_min_samples_split)),\n",
    "        ('rf', RandomForestRegressor(n_estimators=rf_n_estimators, max_depth=rf_max_depth, random_state=42))\n",
    "    ]\n",
    "    \n",
    "    voting_model = VotingRegressor(estimators=estimators)\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', RobustScaler()),  \n",
    "        ('model', voting_model)\n",
    "    ])\n",
    "    \n",
    "    cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    mae_scores = cross_val_score(pipeline, X, y, cv=cv, scoring=make_scorer(mean_absolute_error), n_jobs=-1)\n",
    "    return np.mean(mae_scores)\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "best_trial = study.best_trial\n",
    "print(f\"Best trial params: {best_trial.params}\")\n",
    "print(f\"Best MAE: {best_trial.value:.2f}\")\n",
    "\n",
    "\n",
    "enet_alpha = best_trial.params['enet_alpha']\n",
    "enet_l1_ratio = best_trial.params['enet_l1_ratio']\n",
    "svr_C = best_trial.params['svr_C']\n",
    "svr_epsilon = best_trial.params['svr_epsilon']\n",
    "knn_neighbors = best_trial.params['knn_neighbors']\n",
    "dt_max_depth = best_trial.params['dt_max_depth']\n",
    "dt_min_samples_split = best_trial.params['dt_min_samples_split']\n",
    "rf_n_estimators = best_trial.params['rf_n_estimators']\n",
    "rf_max_depth = best_trial.params['rf_max_depth']\n",
    "\n",
    "\n",
    "estimators = [\n",
    "    ('ridge', Ridge(alpha=1.0)),\n",
    "    ('lasso', Lasso(alpha=0.1)),\n",
    "    ('enet', ElasticNet(alpha=enet_alpha, l1_ratio=enet_l1_ratio, max_iter=10000)),\n",
    "    ('svr', SVR(C=svr_C, epsilon=svr_epsilon, kernel='rbf')),\n",
    "    ('knn', KNeighborsRegressor(n_neighbors=knn_neighbors)),\n",
    "    ('dt', DecisionTreeRegressor(max_depth=dt_max_depth, min_samples_split=dt_min_samples_split)),\n",
    "    ('rf', RandomForestRegressor(n_estimators=rf_n_estimators, max_depth=rf_max_depth, random_state=42))\n",
    "]\n",
    "\n",
    "voting_reg = VotingRegressor(estimators=estimators)\n",
    "stacking_reg = StackingRegressor(estimators=estimators, final_estimator=Ridge(alpha=1.0))\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "voting_pipeline = Pipeline([\n",
    "    ('scaler', RobustScaler()),  \n",
    "    ('model', voting_reg)\n",
    "])\n",
    "voting_mae = cross_val_score(voting_pipeline, X, y, scoring=make_scorer(mean_absolute_error), cv=cv).mean()\n",
    "print(f\"Voting Regressor MAE: {voting_mae}\")\n",
    "\n",
    "stacking_pipeline = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('model', stacking_reg)\n",
    "])\n",
    "stacking_mae = cross_val_score(stacking_pipeline, X, y, scoring=make_scorer(mean_absolute_error), cv=cv).mean()\n",
    "print(f\"Stacking Regressor MAE: {stacking_mae}\")\n",
    "\n",
    "if voting_mae < stacking_mae:\n",
    "    print(\"Using Voting Regressor for final predictions.\")\n",
    "    final_model = voting_pipeline\n",
    "else:\n",
    "    print(\"Using Stacking Regressor for final predictions.\")\n",
    "    final_model = stacking_pipeline\n",
    "\n",
    "final_model.fit(X, y)\n",
    "\n",
    "y_test_pred = final_model.predict(X_test)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_data['id'],\n",
    "    'yield': y_test_pred\n",
    "})\n",
    "submission.to_csv('submission_advanced_model.csv', index=False)\n",
    "print(\"Submission file saved as 'submission_advanced_model.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, RobustScaler\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "from sklearn.linear_model import ElasticNet, Ridge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, VotingRegressor\n",
    "import optuna\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "target_column = 'yield'\n",
    "X = train_data.drop(columns=['id', target_column])\n",
    "y = train_data[target_column]\n",
    "X_test = test_data.drop(columns=['id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X['fruit_density'] = X['seeds'] / (X['fruitmass'] + 1e-3)\n",
    "X_test['fruit_density'] = X_test['seeds'] / (X_test['fruitmass'] + 1e-3)\n",
    "X['seeds_clonesize_interaction'] = X['seeds'] * X['clonesize']\n",
    "X_test['seeds_clonesize_interaction'] = X_test['seeds'] * X_test['clonesize']\n",
    "\n",
    "for col in ['fruitmass', 'seeds', 'RainingDays']:\n",
    "    if col in X.columns:\n",
    "        X[f'log_{col}'] = np.log1p(X[col])\n",
    "        X_test[f'log_{col}'] = np.log1p(X_test[col])\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "important_features = ['fruit_density', 'seeds_clonesize_interaction']\n",
    "X_poly = poly_features.fit_transform(X[important_features])\n",
    "X_poly_df = pd.DataFrame(X_poly, columns=poly_features.get_feature_names_out(important_features))\n",
    "X = pd.concat([X.reset_index(drop=True), X_poly_df], axis=1)\n",
    "\n",
    "X_test_poly = poly_features.transform(X_test[important_features])\n",
    "X_test_poly_df = pd.DataFrame(X_test_poly, columns=poly_features.get_feature_names_out(important_features))\n",
    "X_test = pd.concat([X_test.reset_index(drop=True), X_test_poly_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    enet_alpha = trial.suggest_float('enet_alpha', 1e-4, 1.0, log=True)\n",
    "    enet_l1_ratio = trial.suggest_float('enet_l1_ratio', 0.1, 0.9)\n",
    "    svr_C = trial.suggest_float('svr_C', 0.1, 10.0, log=True)\n",
    "    svr_epsilon = trial.suggest_float('svr_epsilon', 0.01, 0.5)\n",
    "    knn_neighbors = trial.suggest_int('knn_neighbors', 5, 15)\n",
    "\n",
    "    estimators = [\n",
    "        ('ridge', Ridge(alpha=1.0)),\n",
    "        ('enet', ElasticNet(alpha=enet_alpha, l1_ratio=enet_l1_ratio, max_iter=10000)),\n",
    "        ('svr', SVR(C=svr_C, epsilon=svr_epsilon, kernel='rbf')),\n",
    "        ('knn', KNeighborsRegressor(n_neighbors=knn_neighbors)),\n",
    "        ('dt', DecisionTreeRegressor(max_depth=10, min_samples_split=5)),\n",
    "        ('rf', RandomForestRegressor(n_estimators=100, max_depth=15, random_state=42))\n",
    "    ]\n",
    "    voting_model = VotingRegressor(estimators=estimators)\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', RobustScaler()),\n",
    "        ('model', voting_model)\n",
    "    ])\n",
    "    \n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    mae_scores = cross_val_score(pipeline, X, y, cv=cv, scoring=make_scorer(mean_absolute_error), n_jobs=-1)\n",
    "    return np.mean(mae_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-10 08:59:11,244] A new study created in memory with name: no-name-24133d53-3bbc-4dd0-9660-7ecf61453f5a\n",
      "[I 2024-11-10 08:59:42,206] Trial 0 finished with value: 289.18963370653864 and parameters: {'enet_alpha': 0.02679465194841782, 'enet_l1_ratio': 0.6001225703163178, 'svr_C': 1.7841398677694225, 'svr_epsilon': 0.21482629225173266, 'knn_neighbors': 9}. Best is trial 0 with value: 289.18963370653864.\n",
      "[I 2024-11-10 09:00:30,233] Trial 1 finished with value: 280.853918352183 and parameters: {'enet_alpha': 0.045582814271696304, 'enet_l1_ratio': 0.17067576201154244, 'svr_C': 4.105385650975532, 'svr_epsilon': 0.2853857286501716, 'knn_neighbors': 13}. Best is trial 1 with value: 280.853918352183.\n",
      "[I 2024-11-10 09:01:21,318] Trial 2 finished with value: 328.59116091371754 and parameters: {'enet_alpha': 0.36210774997842793, 'enet_l1_ratio': 0.7645201680553573, 'svr_C': 0.2468024284467862, 'svr_epsilon': 0.22042778498334678, 'knn_neighbors': 10}. Best is trial 1 with value: 280.853918352183.\n",
      "[I 2024-11-10 09:02:15,103] Trial 3 finished with value: 278.05098492686625 and parameters: {'enet_alpha': 0.000913848445074099, 'enet_l1_ratio': 0.3557798764634188, 'svr_C': 3.8747244471423765, 'svr_epsilon': 0.021340801494422634, 'knn_neighbors': 13}. Best is trial 3 with value: 278.05098492686625.\n",
      "[I 2024-11-10 09:03:05,157] Trial 4 finished with value: 330.8823851555263 and parameters: {'enet_alpha': 0.1555705414219734, 'enet_l1_ratio': 0.8344504748365119, 'svr_C': 0.11642759263192853, 'svr_epsilon': 0.39423042739316694, 'knn_neighbors': 8}. Best is trial 3 with value: 278.05098492686625.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial params: {'enet_alpha': 0.000913848445074099, 'enet_l1_ratio': 0.3557798764634188, 'svr_C': 3.8747244471423765, 'svr_epsilon': 0.021340801494422634, 'knn_neighbors': 13}\n",
      "Best MAE: 278.05\n"
     ]
    }
   ],
   "source": [
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=5) \n",
    "\n",
    "best_trial = study.best_trial\n",
    "print(f\"Best trial params: {best_trial.params}\")\n",
    "print(f\"Best MAE: {best_trial.value:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "enet_alpha = best_trial.params['enet_alpha']\n",
    "enet_l1_ratio = best_trial.params['enet_l1_ratio']\n",
    "svr_C = best_trial.params['svr_C']\n",
    "svr_epsilon = best_trial.params['svr_epsilon']\n",
    "knn_neighbors = best_trial.params['knn_neighbors']\n",
    "\n",
    "estimators = [\n",
    "    ('ridge', Ridge(alpha=1.0)),\n",
    "    ('enet', ElasticNet(alpha=enet_alpha, l1_ratio=enet_l1_ratio, max_iter=100)),\n",
    "    ('svr', SVR(C=svr_C, epsilon=svr_epsilon, kernel='rbf')),\n",
    "    ('knn', KNeighborsRegressor(n_neighbors=knn_neighbors)),\n",
    "    ('dt', DecisionTreeRegressor(max_depth=10, min_samples_split=5)),\n",
    "    ('rf', RandomForestRegressor(n_estimators=33, max_depth=4, random_state=42))\n",
    "]\n",
    "\n",
    "final_model = VotingRegressor(estimators=estimators)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_pipeline = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('model', final_model)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.261e+09, tolerance: 2.786e+06\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file saved as 'submission_simplified_model.csv'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "final_pipeline.fit(X, y)\n",
    "\n",
    "y_test_pred = final_pipeline.predict(X_test)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_data['id'],\n",
    "    'yield': y_test_pred\n",
    "})\n",
    "submission.to_csv('submission_simplified_model.csv', index=False)\n",
    "print(\"Submission file saved as 'submission_simplified_model.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-10 09:10:39,569] A new study created in memory with name: no-name-387332d7-9baf-4aea-96c7-473caa37eb8e\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, RobustScaler\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, VotingRegressor\n",
    "import optuna\n",
    "\n",
    "# Load data\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Separate features and target\n",
    "target_column = 'yield'\n",
    "X = train_data.drop(columns=['id', target_column])\n",
    "y = train_data[target_column]\n",
    "X_test = test_data.drop(columns=['id'])\n",
    "\n",
    "# Simplified Feature Engineering: Key interactions and log transformations\n",
    "X['fruit_density'] = X['seeds'] / (X['fruitmass'] + 1e-3)\n",
    "X_test['fruit_density'] = X_test['seeds'] / (X_test['fruitmass'] + 1e-3)\n",
    "X['seeds_clonesize_interaction'] = X['seeds'] * X['clonesize']\n",
    "X_test['seeds_clonesize_interaction'] = X_test['seeds'] * X_test['clonesize']\n",
    "\n",
    "# Log transformation for skewed features\n",
    "for col in ['fruitmass', 'seeds', 'RainingDays']:\n",
    "    if col in X.columns:\n",
    "        X[f'log_{col}'] = np.log1p(X[col])\n",
    "        X_test[f'log_{col}'] = np.log1p(X_test[col])\n",
    "\n",
    "# Polynomial Features for selected key interactions\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "important_features = ['fruit_density', 'seeds_clonesize_interaction']\n",
    "X_poly = poly_features.fit_transform(X[important_features])\n",
    "X_poly_df = pd.DataFrame(X_poly, columns=poly_features.get_feature_names_out(important_features))\n",
    "X = pd.concat([X.reset_index(drop=True), X_poly_df], axis=1)\n",
    "\n",
    "X_test_poly = poly_features.transform(X_test[important_features])\n",
    "X_test_poly_df = pd.DataFrame(X_test_poly, columns=poly_features.get_feature_names_out(important_features))\n",
    "X_test = pd.concat([X_test.reset_index(drop=True), X_test_poly_df], axis=1)\n",
    "\n",
    "# Define Optuna objective function for tuning\n",
    "def objective(trial):\n",
    "    # Narrowed parameter search space\n",
    "    ridge_alpha = trial.suggest_float('ridge_alpha', 0.1, 2.0)\n",
    "    svr_C = trial.suggest_float('svr_C', 0.1, 5.0)\n",
    "    rf_max_depth = trial.suggest_int('rf_max_depth', 5, 20)\n",
    "    \n",
    "    # Define models with tuned parameters\n",
    "    estimators = [\n",
    "        ('ridge', Ridge(alpha=ridge_alpha)),\n",
    "        ('lasso', Lasso(alpha=0.1)),  # Fixed alpha for simplicity\n",
    "        ('svr', SVR(C=svr_C, epsilon=0.1, kernel='rbf')),  # Fixed epsilon\n",
    "        ('rf', RandomForestRegressor(n_estimators=100, max_depth=rf_max_depth, random_state=42))\n",
    "    ]\n",
    "    \n",
    "    # Voting Regressor with reduced model set\n",
    "    voting_model = VotingRegressor(estimators=estimators)\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', RobustScaler()),\n",
    "        ('model', voting_model)\n",
    "    ])\n",
    "    \n",
    "    # Cross-validation for MAE scoring\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    mae_scores = cross_val_score(pipeline, X, y, cv=cv, scoring=make_scorer(mean_absolute_error), n_jobs=-1)\n",
    "    return np.mean(mae_scores)\n",
    "\n",
    "# Optimize model using Optuna\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=10)  # Fewer trials for quicker tuning\n",
    "\n",
    "# Retrieve best parameters and model\n",
    "best_trial = study.best_trial\n",
    "print(f\"Best trial params: {best_trial.params}\")\n",
    "print(f\"Best MAE: {best_trial.value:.2f}\")\n",
    "\n",
    "# Extract the best hyperparameters\n",
    "ridge_alpha = best_trial.params['ridge_alpha']\n",
    "svr_C = best_trial.params['svr_C']\n",
    "rf_max_depth = best_trial.params['rf_max_depth']\n",
    "\n",
    "# Final estimators with best params\n",
    "estimators = [\n",
    "    ('ridge', Ridge(alpha=ridge_alpha)),\n",
    "    ('lasso', Lasso(alpha=0.1)),\n",
    "    ('svr', SVR(C=svr_C, epsilon=0.1, kernel='rbf')),\n",
    "    ('rf', RandomForestRegressor(n_estimators=100, max_depth=rf_max_depth, random_state=42))\n",
    "]\n",
    "\n",
    "# Final voting regressor model\n",
    "final_model = VotingRegressor(estimators=estimators)\n",
    "\n",
    "# Final pipeline\n",
    "final_pipeline = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('model', final_model)\n",
    "])\n",
    "\n",
    "# Fit the final model pipeline on the entire training data\n",
    "final_pipeline.fit(X, y)\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_pred = final_pipeline.predict(X_test)\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_data['id'],\n",
    "    'yield': y_test_pred\n",
    "})\n",
    "submission.to_csv('submission_optimized_voting21.csv', index=False)\n",
    "print(\"Submission file saved as 'submission_optimized_voting21.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
