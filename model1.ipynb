{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "12 fits failed out of a total of 60.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "12 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of DecisionTreeRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [-461.42250033           nan -369.19664457           nan           nan\n",
      " -322.47610452 -443.16016051 -289.17739388           nan -361.00319628\n",
      " -357.56411172 -335.89938423 -287.69394809 -337.39590664 -300.12567921\n",
      " -322.47610452 -298.01490957 -335.90753918 -346.92019494 -330.59283504]\n",
      "  warnings.warn(\n",
      "INFO:__main__:Decision Tree MAE on validation set: 277.50\n",
      "INFO:__main__:Switching to Linear Regression as MAE target was not met.\n",
      "INFO:__main__:Linear Regression MAE on validation set: 7211486644.18\n",
      "INFO:__main__:Final model retrained on the entire training dataset.\n",
      "INFO:__main__:Predictions made on the test set.\n",
      "INFO:__main__:Submission file saved as 'submission_simplified_regression.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load data\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Separate features and target\n",
    "X = train_data.drop(columns=['id', 'yield'])\n",
    "y = train_data['yield']\n",
    "X_test = test_data.drop(columns=['id'])\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define models\n",
    "tree_model = DecisionTreeRegressor(random_state=42)\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "# Define the pipeline including polynomial features\n",
    "pipeline = Pipeline([\n",
    "    ('poly_features', PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', tree_model)\n",
    "])\n",
    "\n",
    "# Hyperparameters for RandomizedSearchCV\n",
    "tree_params = {\n",
    "    'model__max_depth': [5, 10, 15, 20, None],\n",
    "    'model__min_samples_split': [2, 5, 10],\n",
    "    'model__min_samples_leaf': [1, 2, 4],\n",
    "    'model__max_features': ['auto', 'sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "# RandomizedSearchCV for decision tree model with polynomial features\n",
    "search = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_distributions=tree_params,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    n_iter=20,  # Increased iterations for better search\n",
    "    cv=3,\n",
    "    random_state=42\n",
    ")\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on validation set\n",
    "best_pipeline = search.best_estimator_\n",
    "y_val_pred = best_pipeline.predict(X_val)\n",
    "mae = mean_absolute_error(y_val, y_val_pred)\n",
    "logger.info(f\"Decision Tree MAE on validation set: {mae:.2f}\")\n",
    "\n",
    "# If MAE target is not met, switch to Linear Regression\n",
    "if mae > 220:\n",
    "    logger.info(\"Switching to Linear Regression as MAE target was not met.\")\n",
    "    linear_model_pipeline = Pipeline([\n",
    "        ('poly_features', PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', linear_model)\n",
    "    ])\n",
    "    linear_model_pipeline.fit(X_train, y_train)\n",
    "    y_val_pred = linear_model_pipeline.predict(X_val)\n",
    "    mae = mean_absolute_error(y_val, y_val_pred)\n",
    "    logger.info(f\"Linear Regression MAE on validation set: {mae:.2f}\")\n",
    "    best_pipeline = linear_model_pipeline if mae <= 220 else search.best_estimator_\n",
    "\n",
    "# Retrain best model on full training data\n",
    "best_pipeline.fit(X, y)\n",
    "logger.info(\"Final model retrained on the entire training dataset.\")\n",
    "\n",
    "# Predictions on the test set\n",
    "y_test_pred = best_pipeline.predict(X_test)\n",
    "logger.info(\"Predictions made on the test set.\")\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_data['id'],\n",
    "    'yield': y_test_pred\n",
    "})\n",
    "submission.to_csv('submission_simplified_regression.csv', index=False)\n",
    "logger.info(\"Submission file saved as 'submission_simplified_regression.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import optuna\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load data\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Separate features and target\n",
    "X = train_data.drop(columns=['id', 'yield'])\n",
    "y = train_data['yield']\n",
    "X_test = test_data.drop(columns=['id'])\n",
    "\n",
    "# Feature Engineering: Adding selective polynomial features\n",
    "selected_features = ['clonesize', 'honeybee', 'bumbles', 'fruitmass']\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_poly = poly.fit_transform(X[selected_features])\n",
    "X_poly_df = pd.DataFrame(X_poly, columns=poly.get_feature_names_out(selected_features))\n",
    "X = pd.concat([X.reset_index(drop=True), X_poly_df], axis=1)\n",
    "X_test_poly = poly.transform(X_test[selected_features])\n",
    "X_test_poly_df = pd.DataFrame(X_test_poly, columns=poly.get_feature_names_out(selected_features))\n",
    "X_test = pd.concat([X_test.reset_index(drop=True), X_test_poly_df], axis=1)\n",
    "\n",
    "# Split the data for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define Optuna objective function\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['auto', 'sqrt', 'log2', None]),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    model = GradientBoostingRegressor(**params)\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    preds = pipeline.predict(X_val)\n",
    "    mae = mean_absolute_error(y_val, preds)\n",
    "    return mae\n",
    "\n",
    "# Optimize model using Optuna\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=30)  # Limited trials for efficiency\n",
    "\n",
    "# Retrieve the best parameters and model\n",
    "best_params = study.best_params\n",
    "best_params['random_state'] = 42\n",
    "logger.info(f\"Best parameters: {best_params}\")\n",
    "\n",
    "# Train the final model with the best parameters\n",
    "final_model = GradientBoostingRegressor(**best_params)\n",
    "final_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', final_model)\n",
    "])\n",
    "\n",
    "# Fit the final model pipeline on the entire training data\n",
    "final_pipeline.fit(X, y)\n",
    "logger.info(\"Final model retrained on the entire training dataset.\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred = final_pipeline.predict(X_test)\n",
    "logger.info(\"Predictions made on the test set.\")\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_data['id'],\n",
    "    'yield': y_test_pred\n",
    "})\n",
    "\n",
    "# Save submission\n",
    "submission.to_csv('submission_advanced_dt_regression.csv', index=False)\n",
    "logger.info(\"Submission file saved as 'submission_advanced_dt_regression.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-09 17:40:05,729] A new study created in memory with name: no-name-7b19c7e8-b47d-48bc-a53d-af69f77c80b1\n",
      "[W 2024-11-09 18:00:57,919] Trial 0 failed with parameters: {'rf_n_estimators': 697, 'rf_max_depth': 43, 'rf_min_samples_split': 5, 'rf_min_samples_leaf': 2, 'rf_max_features': 'sqrt', 'rf_bootstrap': True, 'et_n_estimators': 796, 'et_max_depth': 29, 'et_min_samples_split': 13, 'et_min_samples_leaf': 1, 'et_max_features': 'log2', 'svr_C': 15.615815909493078, 'svr_epsilon': 0.24631264958471535, 'enet_alpha': 0.02496919331080073, 'enet_l1_ratio': 0.5339012783931452} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_3564\\1981592148.py\", line 87, in objective\n",
      "    mae_scores = cross_val_score(pipeline, X, y, cv=cv, scoring=make_scorer(mean_absolute_error), n_jobs=-1)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 213, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 712, in cross_val_score\n",
      "    cv_results = cross_validate(\n",
      "                 ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 213, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 423, in cross_validate\n",
      "    results = parallel(\n",
      "              ^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 74, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 2007, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 1650, in _get_outputs\n",
      "    yield from self._retrieve()\n",
      "  File \"c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 1762, in _retrieve\n",
      "    time.sleep(0.01)\n",
      "KeyboardInterrupt\n",
      "[W 2024-11-09 18:01:04,532] Trial 0 failed with value None.\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import make_scorer, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor, StackingRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "import optuna\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load data\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Separate features and target\n",
    "X = train_data.drop(columns=['id', 'yield'])\n",
    "y = train_data['yield']\n",
    "X_test = test_data.drop(columns=['id'])\n",
    "\n",
    "# Feature Engineering: Adding more polynomial features\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly.fit_transform(X)\n",
    "X_poly_df = pd.DataFrame(X_poly, columns=poly.get_feature_names_out(X.columns))\n",
    "X = pd.concat([X.reset_index(drop=True), X_poly_df], axis=1)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "X_test_poly_df = pd.DataFrame(X_test_poly, columns=poly.get_feature_names_out(X_test.columns))\n",
    "X_test = pd.concat([X_test.reset_index(drop=True), X_test_poly_df], axis=1)\n",
    "\n",
    "# Define Optuna objective function\n",
    "def objective(trial):\n",
    "    # Define hyperparameters for each model\n",
    "    rf_params = {\n",
    "        'n_estimators': trial.suggest_int('rf_n_estimators', 200, 1000),\n",
    "        'max_depth': trial.suggest_int('rf_max_depth', 10, 50),\n",
    "        'min_samples_split': trial.suggest_int('rf_min_samples_split', 2, 15),\n",
    "        'min_samples_leaf': trial.suggest_int('rf_min_samples_leaf', 1, 5),\n",
    "        'max_features': trial.suggest_categorical('rf_max_features', ['sqrt', 'log2', None]),\n",
    "        'bootstrap': trial.suggest_categorical('rf_bootstrap', [True, False]),\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    et_params = {\n",
    "        'n_estimators': trial.suggest_int('et_n_estimators', 200, 1000),\n",
    "        'max_depth': trial.suggest_int('et_max_depth', 10, 50),\n",
    "        'min_samples_split': trial.suggest_int('et_min_samples_split', 2, 15),\n",
    "        'min_samples_leaf': trial.suggest_int('et_min_samples_leaf', 1, 5),\n",
    "        'max_features': trial.suggest_categorical('et_max_features', ['sqrt', 'log2', None]),\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    svr_params = {\n",
    "        'C': trial.suggest_float('svr_C', 0.1, 100.0, log=True),\n",
    "        'epsilon': trial.suggest_float('svr_epsilon', 0.01, 1.0, log=True),\n",
    "        'kernel': 'rbf'\n",
    "    }\n",
    "    enet_params = {\n",
    "        'alpha': trial.suggest_float('enet_alpha', 1e-4, 1.0, log=True),\n",
    "        'l1_ratio': trial.suggest_float('enet_l1_ratio', 0.1, 0.9),\n",
    "        'max_iter': 10000\n",
    "    }\n",
    "    # Define base estimators\n",
    "    estimators = [\n",
    "        ('rf', RandomForestRegressor(**rf_params)),\n",
    "        ('et', ExtraTreesRegressor(**et_params)),\n",
    "        ('svr', SVR(**svr_params)),\n",
    "        ('enet', ElasticNet(**enet_params))\n",
    "    ]\n",
    "    # Define stacking regressor\n",
    "    stacking_model = StackingRegressor(\n",
    "        estimators=estimators,\n",
    "        final_estimator=ElasticNet(max_iter=10000),\n",
    "        n_jobs=-1,\n",
    "        passthrough=True\n",
    "    )\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', stacking_model)\n",
    "    ])\n",
    "    # Cross-validation\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    mae_scores = cross_val_score(pipeline, X, y, cv=cv, scoring=make_scorer(mean_absolute_error), n_jobs=-1)\n",
    "    return np.mean(mae_scores)\n",
    "\n",
    "# Optimize model using Optuna\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)  # Increased trials for better optimization\n",
    "\n",
    "# Retrieve the best parameters and model\n",
    "best_trial = study.best_trial\n",
    "logger.info(f\"Best trial params: {best_trial.params}\")\n",
    "logger.info(f\"Best MAE: {best_trial.value:.2f}\")\n",
    "\n",
    "# Extract the best hyperparameters\n",
    "rf_best_params = {k.replace('rf_', ''): v for k, v in best_trial.params.items() if k.startswith('rf_')}\n",
    "et_best_params = {k.replace('et_', ''): v for k, v in best_trial.params.items() if k.startswith('et_')}\n",
    "svr_best_params = {k.replace('svr_', ''): v for k, v in best_trial.params.items() if k.startswith('svr_')}\n",
    "enet_best_params = {k.replace('enet_', ''): v for k, v in best_trial.params.items() if k.startswith('enet_')}\n",
    "\n",
    "# Define the final estimators with best params\n",
    "rf_best_params['random_state'] = 42\n",
    "rf_best_params['n_jobs'] = -1\n",
    "et_best_params['random_state'] = 42\n",
    "et_best_params['n_jobs'] = -1\n",
    "svr_best_params['kernel'] = 'rbf'\n",
    "enet_best_params['max_iter'] = 10000\n",
    "\n",
    "estimators = [\n",
    "    ('rf', RandomForestRegressor(**rf_best_params)),\n",
    "    ('et', ExtraTreesRegressor(**et_best_params)),\n",
    "    ('svr', SVR(**svr_best_params)),\n",
    "    ('enet', ElasticNet(**enet_best_params))\n",
    "]\n",
    "\n",
    "# Define the final stacking regressor\n",
    "final_model = StackingRegressor(\n",
    "    estimators=estimators,\n",
    "    final_estimator=ElasticNet(max_iter=10000),\n",
    "    n_jobs=-1,\n",
    "    passthrough=True\n",
    ")\n",
    "final_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', final_model)\n",
    "])\n",
    "\n",
    "# Fit the final model pipeline on the entire training data\n",
    "final_pipeline.fit(X, y)\n",
    "logger.info(\"Final model retrained on the entire training dataset.\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred = final_pipeline.predict(X_test)\n",
    "logger.info(\"Predictions made on the test set.\")\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_data['id'],\n",
    "    'yield': y_test_pred\n",
    "})\n",
    "\n",
    "# Save submission\n",
    "submission.to_csv('submission_stacking_regression.csv', index=False)\n",
    "logger.info(\"Submission file saved as 'submission_stacking_regression.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
